{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WaveNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiZAfPdIiIcBAT3Xu62w0W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anton-selitskiy/WaveNet/blob/main/WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ5TTCiKm7Pt",
        "outputId": "8a3ba25a-5476-444c-dba5-d27d2d69c707"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (3.7.4.3)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NlWo7fNnA0_"
      },
      "source": [
        "import math\n",
        "import pathlib\n",
        "import random\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython import display\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import distributions\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MelSpectrogram\n",
        "\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fMhMGDSnYMW"
      },
      "source": [
        "Causal Convolution Block\n",
        "\n",
        "We use nn.ConstantPad1d to add zeros from the left. How to calculate the size of the padding? If the kernal size is M and the length of the input L, then output has length L-M+1. If we want to have the same length, we should add M-1 zeros (then L+M-1 -M+1 = L).\n",
        "\n",
        "For example, create a tensor:\n",
        "\n",
        "```\n",
        "batch_size = 1\n",
        "in_channel = 1\n",
        "time =10\n",
        "inp = torch.arange(time).reshape(batch_size, in_channel, time).float()\n",
        "#inp = torch.rand(batch_size, in_channel, time)\n",
        "inp\n",
        "```\n",
        "tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
        "\n",
        "Create a kernal:\n",
        "```\n",
        "kernel_size = 2\n",
        "padding_f = nn.ConstantPad1d((kernel_size-1,0), value=0.0)\n",
        "conv = nn.Conv1d(1,1,kernel_size,bias=False,dilation=1)\n",
        "conv.weight.data = torch.ones(1,1,kernel_size)\n",
        "conv.weight.data\n",
        "```\n",
        "tensor([[[1., 1.]]])\n",
        "```\n",
        "print(padding_f(inp))\n",
        "print(conv(padding_f(inp)))\n",
        "```\n",
        "tensor([[[0., 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
        "\n",
        "tensor([[[ 0.,  1.,  3.,  5.,  7.,  9., 11., 13., 15., 17.]]],\n",
        "       grad_fn=\\<SqueezeBackward1\\>)\n",
        "\n",
        "As a result, two nearest digits were addad with weights 1 and the length of the output did not change.\n",
        "\n",
        "If we want to add a dilation (dilatation) D, then we should add (M-1)*d zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ0XP2dW9N-P",
        "outputId": "13f08596-1ef7-4161-f41e-9b0a24ae6a06"
      },
      "source": [
        "batch_size = 1\n",
        "in_channel = 1\n",
        "time =10\n",
        "inp = torch.arange(time).reshape(batch_size, in_channel, time).float()\n",
        "# inp = torch.rand(batch_size, in_channel, time)\n",
        "print(inp)\n",
        "kernel_size = 3\n",
        "padding_f = nn.ConstantPad1d((kernel_size-1,0), value=0.0)\n",
        "conv = nn.Conv1d(1,1,kernel_size,bias=False,dilation=2)\n",
        "conv.weight.data = torch.ones(1,1,kernel_size)\n",
        "print(conv.weight.data)\n",
        "print(padding_f(inp))\n",
        "print(conv(padding_f(inp)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
            "tensor([[[1., 1., 1.]]])\n",
            "tensor([[[0., 0., 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
            "tensor([[[ 2.,  4.,  6.,  9., 12., 15., 18., 21.]]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQZGEm7inOnx"
      },
      "source": [
        "class CausalConv1d(nn.Conv1d):\n",
        "    \"\"\"\n",
        "    Casual Conv1d\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        dilation: int = 1,\n",
        "        bias: bool = True\n",
        "    ):\n",
        "        super().__init__(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            dilation=dilation,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        padding_size = (kernel_size - 1) * dilation\n",
        "        self.zero_padding = nn.ConstantPad1d(\n",
        "            padding=(padding_size, 0),\n",
        "            value=0.0\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        padded_input = self.zero_padding(input)\n",
        "        output = super().forward(padded_input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4koOkopneqk"
      },
      "source": [
        "### Gated Activation Unit\n",
        "$tanh(W_f*x )\\cdot \\sigma(W_g*x)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA2KoRDJnUsQ"
      },
      "source": [
        "class GatedConv1d(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Conv1d\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        dilation: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.filter_conv = CausalConv1d(in_channels, out_channels, kernel_size, dilation)\n",
        "        self.gate_conv = CausalConv1d(in_channels, out_channels, kernel_size, dilation)\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        filter_ouput = self.filter_conv(input)\n",
        "        gate_output = self.gate_conv(input)\n",
        "        \n",
        "        output = torch.tanh(filter_ouput) * torch.sigmoid(gate_output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjNlfv1KzwSb"
      },
      "source": [
        "Conditioned GAU $tanh(W_f*x + V_f*x)\\cdot \\sigma(W_g*x+V_g*x)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL2B076yA8fw"
      },
      "source": [
        "class CondGatedConv1d(GatedConv1d):\n",
        "    \"\"\"\n",
        "    Conditioned Gated Conv1d\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        cond_in_channels: int,\n",
        "        kernel_size: int,\n",
        "        dilation: int\n",
        "    ):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, dilation)\n",
        "\n",
        "        self.cond_conv = nn.Conv1d(\n",
        "            in_channels=cond_in_channels,\n",
        "            out_channels=2 * out_channels,\n",
        "            kernel_size=1\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input: torch.Tensor,\n",
        "        condition: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        assert input.size(-1) == condition.size(-1)\n",
        "        \n",
        "        filter_ouput = self.filter_conv(input)\n",
        "        gate_output = self.gate_conv(input)\n",
        "        \n",
        "        c_output = self.cond_conv(condition)\n",
        "        # We can use  2 channels instead of two filters (function chunk):\n",
        "        c_filter_output, c_gate_output = torch.chunk(c_output, 2, dim=1)\n",
        "        \n",
        "        output = torch.tanh(filter_ouput + c_filter_output) * torch.sigmoid(gate_output + c_gate_output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azvu9-6b1OS0"
      },
      "source": [
        "Example of using chunk\n",
        "```\n",
        "torch.chunk(inp, 2, dim=-1)\n",
        "```\n",
        "(tensor([[[0., 1., 2., 3., 4.]]]), tensor([[[5., 6., 7., 8., 9.]]]))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm1hy4yHnRUA",
        "outputId": "41de8d02-27c7-450f-c38f-8aeb6fe6e960"
      },
      "source": [
        "torch.chunk(inp, 2, dim=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0., 1., 2., 3., 4.]]]), tensor([[[5., 6., 7., 8., 9.]]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptjy08cQ2E91"
      },
      "source": [
        "### Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBe74XFl1HdR"
      },
      "source": [
        "class CondWaveNetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Conditioned WaveNet block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        gated_in_channels: int,\n",
        "        gated_out_channels: int,\n",
        "        cond_in_channels: int,\n",
        "        skip_out_channels: int,\n",
        "        kernel_size: int,\n",
        "        dilation: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.gated_cond = CondGatedConv1d(\n",
        "            in_channels=gated_in_channels,\n",
        "            out_channels=gated_out_channels,\n",
        "            cond_in_channels=cond_in_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            dilation=dilation\n",
        "        )\n",
        "\n",
        "        self.skip_conv = nn.Conv1d(gated_out_channels, skip_out_channels, kernel_size=1)\n",
        "        self.residual_conv = nn.Conv1d(gated_out_channels, gated_in_channels, kernel_size=1)\n",
        "    \n",
        "\n",
        "    def forward(self, input: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:\n",
        "        gated_output = self.gated_cond(input, condition)\n",
        "        \n",
        "        # y = f(x) + x\n",
        "        residual_output = self.residual_conv(gated_output) + input\n",
        "        skip_output = self.skip_conv(gated_output)\n",
        "\n",
        "        return residual_output, skip_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JJTik9w8DQE"
      },
      "source": [
        "### Reduce quantisation size from $2^{16}$ to $2^8$\n",
        "$f(x) = sign(x) \\dfrac{\\ln(1+\\mu |x|)}{\\ln(1+\\mu)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knzsv-5U2CwJ"
      },
      "source": [
        "class MuLaw(nn.Module):\n",
        "\n",
        "    def __init__(self, mu: float = 256):\n",
        "        super().__init__()\n",
        "        self.register_buffer('mu', torch.FloatTensor([mu - 1]))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.encode(input)\n",
        "\n",
        "    def encode(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        input = torch.clamp(input, -1 + 1e-5, 1 - 1e-5)\n",
        "\n",
        "        mu_law_output = torch.sign(input) * torch.log1p(self.mu * torch.abs(input)) / torch.log1p(self.mu)\n",
        "\n",
        "        # [-1, 1] -> [0, 1]\n",
        "        quantized_output = (mu_law_output + 1) / 2\n",
        "\n",
        "        # [0, 1] -> [0, mu - 1]\n",
        "        quantized_output = torch.floor(quantized_output * self.mu + 0.5).long()\n",
        "        \n",
        "        return quantized_output\n",
        "\n",
        "    def decode(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        dequantized_output = (input.float() / self.mu) * 2 - 1\n",
        "        output = (torch.sign(dequantized_output) / self.mu) * \\\n",
        "            ((1 + self.mu) ** torch.abs(dequantized_output) - 1)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBcVlEl_EmTc"
      },
      "source": [
        "```\n",
        "mu_law_encoder = MuLaw(256)\n",
        "input = torch.randn(5).mul(0.1).clamp(-1, 1)\n",
        "print(f'Input: {input}')\n",
        "print(f'After MuLaw Encoding: {mu_law_encoder(input)}')\n",
        "print(f'After MuLaw Decoding: {mu_law_encoder.decode(mu_law_encoder(input))}')\n",
        "\n",
        "Input: tensor([ 0.1020, -0.0206, -0.0404,  0.0036,  0.2439])\n",
        "\n",
        "After MuLaw Encoding: tensor([203,  85,  72, 142, 223])\n",
        "\n",
        "After MuLaw Decoding: tensor([ 0.1007, -0.0210, -0.0399,  0.0034,  0.2457])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM_5AML7EYiX"
      },
      "source": [
        "class OneHot(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert quantized 1d samples into n_class one-hot tensor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_class: int = 256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_class = n_class\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        assert input.dim() == 3, \"Expected shape of input is [B, C, T], where C == 1\"\n",
        "        return self.encode(input)\n",
        "\n",
        "    def encode(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        output = torch.zeros(input.size(0), self.n_class, input.size(-1), device=input.device)\n",
        "        output.scatter_(1, input, 1)\n",
        "        return output\n",
        "\n",
        "    def decode(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7p_vSoU1v4L"
      },
      "source": [
        "```\n",
        "upsampling = nn.Upsample(scale_factor=3, mode='nearest')\n",
        "input = torch.arange(5).view(1, 1, -1).float()\n",
        "print(f'Input: {input.squeeze()}')\n",
        "print(f'After Upsampling: {upsampling(input).squeeze()}')\n",
        "\n",
        "Input: tensor([0., 1., 2., 3., 4.])\n",
        "After Upsampling: tensor([0., 0., 0., 1., 1., 1., 2., 2., 2., 3., 3., 3., 4., 4., 4.])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR6U4GXU1wg3"
      },
      "source": [
        "class CondNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Processing condition (mel from TTS or something else)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int, hop_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hop_size = hop_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.net = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size // 2,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.upsampler = nn.Upsample(scale_factor=hop_size, mode='nearest')\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :return: .shape == [B, T', C']\n",
        "        \"\"\"\n",
        "        \n",
        "        assert input.shape[-1] == self.input_size\n",
        "\n",
        "        self.net.flatten_parameters()\n",
        "\n",
        "        output, _ = self.net(input)\n",
        "\n",
        "        output = output.transpose(-1, -2)\n",
        "        upsampled_output = self.upsampler(output)\n",
        "\n",
        "        return upsampled_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOejQhmMPgvs"
      },
      "source": [
        "WaveNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "x7CyMtzSPiVf",
        "outputId": "abf1a78e-3466-4f62-8f0b-350190d71f32"
      },
      "source": [
        "class WaveNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        # in_channels: int = 256,\n",
        "        # out_channels: int = 256,\n",
        "        # gate_channels: int = 512,\n",
        "        # residual_channels: int = 256,\n",
        "        # skip_channels: int = 256,\n",
        "        # head_channels: int = 256,\n",
        "        # condition_channels: int = 256,\n",
        "        \n",
        "        in_channels: int = 64,\n",
        "        out_channels: int = 64,\n",
        "        gate_channels: int = 64,\n",
        "        residual_channels: int = 64,\n",
        "        skip_channels: int = 64,\n",
        "        head_channels: int = 64,\n",
        "        condition_channels: int = 64,\n",
        "        kernel_size: int = 2,\n",
        "        dilation_cycles: int = 3,\n",
        "        dilation_depth: int = 10,\n",
        "        upsample_factor: int = 480,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.gate_channels = gate_channels\n",
        "        self.residual_channels = residual_channels\n",
        "        self.skip_channels = skip_channels\n",
        "        self.head_channels = head_channels\n",
        "        self.condition_channels = condition_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation_cycles = dilation_cycles\n",
        "        self.dilation_depth = dilation_depth\n",
        "        self.upsample_factor = upsample_factor\n",
        "\n",
        "        # 80 -- number of channels in mels \n",
        "        self.cond = CondNet(80, self.condition_channels, upsample_factor)\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, residual_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            CondWaveNetBlock(residual_channels, gate_channels, condition_channels, skip_channels,\n",
        "                             kernel_size, 2 ** (i % dilation_depth))\n",
        "            for i in range(dilation_cycles * dilation_depth)\n",
        "        ])\n",
        "\n",
        "        # To avoid DDP error\n",
        "        self.blocks[-1].residual_conv.requires_grad_(False)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(skip_channels, head_channels, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(head_channels, out_channels, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def _forward(self, input: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:\n",
        "        # already upsampled condition\n",
        "\n",
        "        stem_output = self.stem(input)\n",
        "\n",
        "        accumulation = 0\n",
        "        residual_output = stem_output\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            residual_output, skip_output = block(residual_output, condition)\n",
        "            accumulation = accumulation + skip_output\n",
        "\n",
        "        output = self.head(accumulation)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, input: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param input: samples\n",
        "        :param condition: mel\n",
        "        \"\"\"\n",
        "\n",
        "        condition = self.cond(condition)\n",
        "        output = self._forward(input, condition)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @property\n",
        "    def num_parameters(self) -> int:\n",
        "        return sum([p.numel() for p in self.parameters()])\n",
        "\n",
        "    @property\n",
        "    def receptive_field(self) -> int:\n",
        "        dilations = [2 ** (i % self.dilation_depth)\n",
        "                     for i in range(self.dilation_cycles * self.dilation_depth)]\n",
        "        receptive_field = (self.kernel_size - 1) * sum(dilations) + 1\n",
        "\n",
        "        return receptive_field\n",
        "\n",
        "    def generate(self, condition: torch.Tensor, inference_type: str = \"naive\", verbose: bool = True) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param condition: [1, T, C], C from ASR\n",
        "        :param inference_type:\n",
        "        :param verbose:\n",
        "        \"\"\"\n",
        "\n",
        "        mu_low = MuLaw().to(condition.device)\n",
        "\n",
        "        if inference_type == \"naive\":\n",
        "            compressed_samples = self._naive_generate(condition, verbose)\n",
        "        elif inference_type == \"fast\":\n",
        "            compressed_samples = self._fast_generate(condition)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid type of inference: {inference_type}\")\n",
        "\n",
        "        return mu_low.decode(compressed_samples)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _naive_generate(self, condition: torch.Tensor, verbose: bool) -> torch.Tensor:\n",
        "        one_hot = OneHot()\n",
        "\n",
        "        required_num_samples = condition.shape[1] * self.upsample_factor\n",
        "        generated_samples = torch.Tensor(1, 1, self.receptive_field + required_num_samples) \\\n",
        "            .fill_(self.in_channels // 2) \\\n",
        "            .to(condition.device)\n",
        "\n",
        "        condition = self.cond(condition)\n",
        "        condition = F.pad(condition, (self.receptive_field, 0), 'replicate')\n",
        "\n",
        "        iterator = range(required_num_samples)\n",
        "        if verbose:\n",
        "            iterator = tqdm(iterator)\n",
        "\n",
        "        for i in iterator:\n",
        "            current_condition = condition[:, :, i:i + self.receptive_field]\n",
        "            current_samples = generated_samples[:, :, i:i + self.receptive_field]\n",
        "            current_one_hot_samples = one_hot(current_samples.long())\n",
        "\n",
        "            current_output = self._forward(current_one_hot_samples, current_condition)\n",
        "            last_logits = current_output[:, :, -1].squeeze()\n",
        "\n",
        "            # sampling new sample\n",
        "            samples = distributions.Categorical(logits=last_logits)\n",
        "            new_sample = samples.sample(torch.Size([1]))\n",
        "            generated_samples[:, :, i + self.receptive_field] = new_sample\n",
        "\n",
        "        return generated_samples.squeeze()[-required_num_samples:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a98f700f0947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mWaveNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# in_channels: int = 256,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZKO7_O4P8ys"
      },
      "source": [
        "model = WaveNet()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}